{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chithsabesh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\externals\\six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
      "C:\\Users\\Chithsabesh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "attacks = pd.read_csv('attacks_cleaned_activity.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'Case Number', 'Date', 'Year', 'Type', 'Country', 'Area',\n",
       "       'Location', 'Activity', 'Name', 'Sex ', 'Age', 'Injury', 'Is_Fatal',\n",
       "       'Investigator or Source', 'col', 'Month', 'Activity_new', 'Count'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attacks.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For supervised machine learning lets take Is_Fatal as the target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map everything as unprovoked in attack type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "attacks['Type'] = attacks['Type'].replace({'Boat':'Unprovoked','Invalid':'Unprovoked','Boating':'Unprovoked','Sea Disaster':'Unprovoked'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unprovoked    3447\n",
       "Provoked       313\n",
       "Name: Type, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attacks['Type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "attacks['Sex '] = attacks['Sex '].replace({'M ':'M','N':'F','lli':'F'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M    3318\n",
       "F     442\n",
       "Name: Sex , dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attacks['Sex '].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets classify fatalities without using text column Injury"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "attacks_features = attacks[['Type','Sex ','Age','Is_Fatal','Month','Country','Activity_new']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Since Month is actually a category lets replace them manually first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chithsabesh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "attacks_features['Month'] = attacks_features['Month'].replace({1:'January',2:'February',3:'March',4:'April',5:'May',6:'June',7:'July',8:'August'\n",
    "                                            ,9:'September',10:'October',11:'November',12:'December'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "July         444\n",
       "August       417\n",
       "September    366\n",
       "June         332\n",
       "January      326\n",
       "October      303\n",
       "April        291\n",
       "December     281\n",
       "March        256\n",
       "November     253\n",
       "May          247\n",
       "February     244\n",
       "Name: Month, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attacks_features['Month'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3760 entries, 0 to 3759\n",
      "Data columns (total 7 columns):\n",
      "Type            3760 non-null object\n",
      "Sex             3760 non-null object\n",
      "Age             3760 non-null float64\n",
      "Is_Fatal        3760 non-null object\n",
      "Month           3760 non-null object\n",
      "Country         3760 non-null object\n",
      "Activity_new    3760 non-null object\n",
      "dtypes: float64(1), object(6)\n",
      "memory usage: 205.8+ KB\n"
     ]
    }
   ],
   "source": [
    "attacks_features.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Since there is no hierarchy we must one hot encode the categories if they were ranked we can label encode them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Type', 'Sex ', 'Is_Fatal', 'Month', 'Country', 'Activity_new'], dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_df=attacks_features.select_dtypes(include='object')\n",
    "cat_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "N          2966\n",
       "Y           779\n",
       "UNKNOWN      11\n",
       " N            4\n",
       "Name: Is_Fatal, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attacks_features['Is_Fatal'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chithsabesh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "attacks_features['Is_Fatal'] = attacks_features['Is_Fatal'].replace({' N':'N','UNKNOWN':'Y'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "N    2970\n",
       "Y     790\n",
       "Name: Is_Fatal, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attacks_features['Is_Fatal'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "N    78.989362\n",
       "Y    21.010638\n",
       "Name: Is_Fatal, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attacks_features['Is_Fatal'].value_counts()/attacks_features.shape[0]*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imbalanced class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3760, 150)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attacks_dummy      = pd.get_dummies(attacks_features.drop('Is_Fatal',axis = 1))\n",
    "attacks_dummy['Is_Fatal'] = attacks_features['Is_Fatal']\n",
    "attacks_dummy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2632, 150)\n",
      "(1128, 150)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2632, 149)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train,test = train_test_split(attacks_dummy,test_size=0.3,\n",
    "                             random_state = 100)\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "train_y = train['Is_Fatal']\n",
    "test_y  = test['Is_Fatal']\n",
    "train_x = train.drop(['Is_Fatal'], axis = 1)#axis 1 is for row wise operation\n",
    "test_x  = test.drop (['Is_Fatal'], axis = 1)\n",
    "train_x.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We use synthetic minority oversampling technique for dealing with imbalanced classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chithsabesh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4174, 149)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Handling imbalanced class\n",
    "oversampler=SMOTE(random_state=42)\n",
    "x_train_smote,  y_train_smote = oversampler.fit_sample(train_x,train_y)\n",
    "x_train_smote.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chithsabesh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(random_state=1)\n",
    "logreg.fit(x_train_smote,y_train_smote)\n",
    "log_pred  = logreg.predict(test_x)\n",
    "df_logreg   = pd.DataFrame({'actual': test_y,\n",
    "                         'predicted': log_pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160 644 85 239\n"
     ]
    }
   ],
   "source": [
    "#MANUALLY CHECKING \n",
    "tp_log = df_logreg[(df_logreg['predicted']=='Y') & (df_logreg['actual']=='Y')].shape[0]\n",
    "tn_log = df_logreg[(df_logreg['predicted']=='N') & (df_logreg['actual']=='N')].shape[0]\n",
    "fp_log = df_logreg[(df_logreg['predicted']=='N') & (df_logreg['actual']=='Y')].shape[0]\n",
    "fn_log = df_logreg[(df_logreg['predicted']=='Y') & (df_logreg['actual']=='N')].shape[0]\n",
    "print(tp_log,tn_log,fp_log,fn_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[644 239]\n",
      " [ 85 160]]\n"
     ]
    }
   ],
   "source": [
    "#Check confusion matrix\n",
    "print(confusion_matrix(test_y,log_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40100250626566414\n",
      "0.8834019204389575\n",
      "0.6530612244897959\n",
      "0.4968944099378882\n"
     ]
    }
   ],
   "source": [
    "sensitivity_log = tp_log/(tp_log+fn_log)\n",
    "print(sensitivity_log)\n",
    "specificity_log = tn_log/(tn_log+fp_log)\n",
    "print(specificity_log)\n",
    "precision_log = tp_log/(tp_log+fp_log)\n",
    "print(precision_log)\n",
    "f1_score = 2*(precision_log*sensitivity_log)/(precision_log+sensitivity_log)\n",
    "print(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TP             160.000000\n",
       "TN             644.000000\n",
       "FP              85.000000\n",
       "FN             239.000000\n",
       "SENSITIVITY      0.401003\n",
       "SPECIFICITY      0.883402\n",
       "Precision        0.653061\n",
       "f1_score         0.496894\n",
       "dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logisticregression = pd.Series({'TP': tp_log, 'TN' : tn_log, 'FP': fp_log, 'FN': fn_log, 'SENSITIVITY': sensitivity_log,\n",
    "                  'SPECIFICITY': specificity_log,'Precision':precision_log,'f1_score':f1_score})\n",
    "logisticregression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_decisiontree = DecisionTreeClassifier(random_state = 1)\n",
    "model_decisiontree\n",
    "model_decisiontree.fit(x_train_smote,y_train_smote)\n",
    "decisiontree_pred = model_decisiontree.predict(test_x)\n",
    "df_decisiontree = pd.DataFrame({'actual':test_y,\n",
    "                        'predicted': decisiontree_pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89 762 156 121\n"
     ]
    }
   ],
   "source": [
    "#MANUALLY CHECKING \n",
    "tp_dt = df_decisiontree[(df_decisiontree['predicted']=='Y') & (df_decisiontree['actual']=='Y')].shape[0]\n",
    "tn_dt = df_decisiontree[(df_decisiontree['predicted']=='N') & (df_decisiontree['actual']=='N')].shape[0]\n",
    "fp_dt = df_decisiontree[(df_decisiontree['predicted']=='N') & (df_decisiontree['actual']=='Y')].shape[0]\n",
    "fn_dt = df_decisiontree[(df_decisiontree['predicted']=='Y') & (df_decisiontree['actual']=='N')].shape[0]\n",
    "print(tp_dt,tn_dt,fp_dt,fn_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4238095238095238\n",
      "0.8300653594771242\n",
      "0.363265306122449\n",
      "0.3912087912087912\n"
     ]
    }
   ],
   "source": [
    "sensitivity_dt = tp_dt/(tp_dt+fn_dt)\n",
    "print(sensitivity_dt)\n",
    "specificity_dt = tn_dt/(tn_dt+fp_dt)\n",
    "print(specificity_dt)\n",
    "precision_dt = tp_dt/(tp_dt+fp_dt)\n",
    "print(precision_dt)\n",
    "f1_score_dt = 2*(precision_dt*sensitivity_dt)/(precision_dt+sensitivity_dt)\n",
    "print(f1_score_dt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TP              89.000000\n",
       "TN             762.000000\n",
       "FP             156.000000\n",
       "FN             121.000000\n",
       "SENSITIVITY      0.423810\n",
       "SPECIFICITY      0.830065\n",
       "Precision        0.363265\n",
       "f1_score         0.391209\n",
       "dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decisiontree = pd.Series({'TP': tp_dt, 'TN' : tn_dt, 'FP': fp_dt, 'FN': fn_dt, 'SENSITIVITY': sensitivity_dt,\n",
    "                  'SPECIFICITY': specificity_dt,'Precision':precision_dt,'f1_score':f1_score_dt})\n",
    "decisiontree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rf = RandomForestClassifier(random_state=1)\n",
    "model_rf.fit(x_train_smote,y_train_smote)\n",
    "\n",
    "test_pred_rf = model_rf.predict(test_x)\n",
    "df_rf   = pd.DataFrame({'actual': test_y,\n",
    "                         'predicted': test_pred_rf})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79 789 166 94\n"
     ]
    }
   ],
   "source": [
    "tp_rf = df_rf[(df_rf['predicted']=='Y') & (df_rf['actual']=='Y')].shape[0]\n",
    "tn_rf = df_rf[(df_rf['predicted']=='N') & (df_rf['actual']=='N')].shape[0]\n",
    "fp_rf = df_rf[(df_rf['predicted']=='N') & (df_rf['actual']=='Y')].shape[0]\n",
    "fn_rf = df_rf[(df_rf['predicted']=='Y') & (df_rf['actual']=='N')].shape[0]\n",
    "print(tp_rf,tn_rf,fp_rf,fn_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45664739884393063\n",
      "0.8261780104712042\n",
      "0.3224489795918367\n",
      "0.3779904306220095\n"
     ]
    }
   ],
   "source": [
    "sensitivity_rf = tp_rf/(tp_rf+fn_rf)\n",
    "print(sensitivity_rf)\n",
    "specificity_rf = tn_rf/(tn_rf+fp_rf)\n",
    "print(specificity_rf)\n",
    "precision_rf = tp_rf/(tp_rf+fp_rf)\n",
    "print(precision_rf)\n",
    "f1_score_rf=2*(precision_rf*sensitivity_rf)/(precision_rf+sensitivity_rf)\n",
    "print(f1_score_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TP              79.000000\n",
       "TN             789.000000\n",
       "FP             166.000000\n",
       "FN              94.000000\n",
       "SENSITIVITY      0.456647\n",
       "SPECIFICITY      0.826178\n",
       "Precision        0.322449\n",
       "f1_score         0.377990\n",
       "dtype: float64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomforest = pd.Series({'TP': tp_rf, 'TN' : tn_rf, 'FP': fp_rf, 'FN': fn_rf, 'SENSITIVITY': sensitivity_rf,\n",
    "                  'SPECIFICITY': specificity_rf,'Precision':precision_rf,'f1_score':f1_score_rf})\n",
    "randomforest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ada = AdaBoostClassifier(random_state=1)\n",
    "model_ada.fit(x_train_smote,y_train_smote)\n",
    "test_pred_abm = model_ada.predict(test_x)\n",
    "df_abm  = pd.DataFrame({'actual': test_y,\n",
    "                         'predicted': test_pred_abm})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132 731 113 152\n"
     ]
    }
   ],
   "source": [
    "tp_ada = df_abm[(df_abm['predicted']=='Y') & (df_abm['actual']=='Y')].shape[0]\n",
    "tn_ada = df_abm[(df_abm['predicted']=='N')& (df_abm['actual']=='N')].shape[0]\n",
    "fp_ada = df_abm[(df_abm['predicted']=='N') & (df_abm['actual']=='Y')].shape[0]\n",
    "fn_ada = df_abm[(df_abm['predicted']=='Y') & (df_abm['actual']=='N')].shape[0]\n",
    "print(tp_ada,tn_ada,fp_ada,fn_ada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4647887323943662\n",
      "0.8661137440758294\n",
      "0.5387755102040817\n",
      "0.49905482041587895\n"
     ]
    }
   ],
   "source": [
    "sensitivity_abm = tp_ada/(tp_ada+fn_ada)\n",
    "print(sensitivity_abm)\n",
    "specificity_abm = tn_ada/(tn_ada+fp_ada)\n",
    "print(specificity_abm)\n",
    "precision_abm = tp_ada/(tp_ada+fp_ada)\n",
    "print(precision_abm)\n",
    "f1_score_abm=2*(precision_abm*sensitivity_abm)/(precision_abm+sensitivity_abm)\n",
    "print(f1_score_abm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TP             132.000000\n",
       "TN             731.000000\n",
       "FP             113.000000\n",
       "FN             152.000000\n",
       "SENSITIVITY      0.464789\n",
       "SPECIFICITY      0.866114\n",
       "Precision        0.538776\n",
       "f1_score         0.499055\n",
       "dtype: float64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_abm = pd.Series({'TP': tp_ada, 'TN' : tn_ada, 'FP': fp_ada, 'FN': fn_ada, 'SENSITIVITY': sensitivity_abm,\n",
    "                  'SPECIFICITY': specificity_abm,'Precision':precision_abm,'f1_score':f1_score_abm})\n",
    "s_abm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = test_x.values\n",
    "xgb = XGBClassifier()\n",
    "xgb.fit(x_train_smote,y_train_smote)\n",
    "test_pred_xgb = xgb.predict(test_x)\n",
    "df_xgb  = pd.DataFrame({'actual': test_y,\n",
    "                         'predicted': test_pred_xgb})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110 773 136 110\n"
     ]
    }
   ],
   "source": [
    "tp_xgb = df_xgb[(df_xgb['predicted']=='Y') & (df_xgb['actual']=='N')].shape[0]\n",
    "tn_xgb = df_xgb[(df_xgb['predicted']=='N')& (df_xgb['actual']=='N')].shape[0]\n",
    "fp_xgb = df_xgb[(df_xgb['predicted']=='N') & (df_xgb['actual']=='Y')].shape[0]\n",
    "fn_xgb = df_xgb[(df_xgb['predicted']=='Y') & (df_xgb['actual']=='N')].shape[0]\n",
    "print(tp_xgb,tn_xgb,fp_xgb,fn_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.8503850385038504\n",
      "0.44715447154471544\n",
      "0.4721030042918455\n"
     ]
    }
   ],
   "source": [
    "sensitivity_xgb = tp_xgb/(tp_xgb+fn_xgb)\n",
    "print(sensitivity_xgb)\n",
    "specificity_xgb = tn_xgb/(tn_xgb+fp_xgb)\n",
    "print(specificity_xgb)\n",
    "\n",
    "precision_xgb = tp_xgb/(tp_xgb+fp_xgb)\n",
    "print(precision_xgb)\n",
    "f1_score_xgb = 2*(precision_xgb*sensitivity_xgb)/(precision_xgb+sensitivity_xgb)\n",
    "print(f1_score_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TP             110.000000\n",
       "TN             773.000000\n",
       "FP             136.000000\n",
       "FN             110.000000\n",
       "SENSITIVITY      0.500000\n",
       "SPECIFICITY      0.850385\n",
       "Precision        0.447154\n",
       "f1_score         0.472103\n",
       "dtype: float64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_xgb = pd.Series({'TP': tp_xgb, 'TN' : tn_xgb, 'FP': fp_xgb, 'FN': fn_xgb, 'SENSITIVITY': sensitivity_xgb,\n",
    "                  'SPECIFICITY': specificity_xgb,'Precision':precision_xgb,'f1_score':f1_score_xgb})\n",
    "s_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Logistic Regression</th>\n",
       "      <th>Decision Tree</th>\n",
       "      <th>Random Forest</th>\n",
       "      <th>Ada Boost</th>\n",
       "      <th>XGBOOST</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TP</th>\n",
       "      <td>160.000000</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>132.000000</td>\n",
       "      <td>110.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TN</th>\n",
       "      <td>644.000000</td>\n",
       "      <td>762.000000</td>\n",
       "      <td>789.000000</td>\n",
       "      <td>731.000000</td>\n",
       "      <td>773.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FP</th>\n",
       "      <td>85.000000</td>\n",
       "      <td>156.000000</td>\n",
       "      <td>166.000000</td>\n",
       "      <td>113.000000</td>\n",
       "      <td>136.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FN</th>\n",
       "      <td>239.000000</td>\n",
       "      <td>121.000000</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>152.000000</td>\n",
       "      <td>110.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SENSITIVITY</th>\n",
       "      <td>0.401003</td>\n",
       "      <td>0.423810</td>\n",
       "      <td>0.456647</td>\n",
       "      <td>0.464789</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SPECIFICITY</th>\n",
       "      <td>0.883402</td>\n",
       "      <td>0.830065</td>\n",
       "      <td>0.826178</td>\n",
       "      <td>0.866114</td>\n",
       "      <td>0.850385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.653061</td>\n",
       "      <td>0.363265</td>\n",
       "      <td>0.322449</td>\n",
       "      <td>0.538776</td>\n",
       "      <td>0.447154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1_score</th>\n",
       "      <td>0.496894</td>\n",
       "      <td>0.391209</td>\n",
       "      <td>0.377990</td>\n",
       "      <td>0.499055</td>\n",
       "      <td>0.472103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Logistic Regression  Decision Tree  Random Forest   Ada Boost  \\\n",
       "TP                    160.000000      89.000000      79.000000  132.000000   \n",
       "TN                    644.000000     762.000000     789.000000  731.000000   \n",
       "FP                     85.000000     156.000000     166.000000  113.000000   \n",
       "FN                    239.000000     121.000000      94.000000  152.000000   \n",
       "SENSITIVITY             0.401003       0.423810       0.456647    0.464789   \n",
       "SPECIFICITY             0.883402       0.830065       0.826178    0.866114   \n",
       "Precision               0.653061       0.363265       0.322449    0.538776   \n",
       "f1_score                0.496894       0.391209       0.377990    0.499055   \n",
       "\n",
       "                XGBOOST  \n",
       "TP           110.000000  \n",
       "TN           773.000000  \n",
       "FP           136.000000  \n",
       "FN           110.000000  \n",
       "SENSITIVITY    0.500000  \n",
       "SPECIFICITY    0.850385  \n",
       "Precision      0.447154  \n",
       "f1_score       0.472103  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_complete = pd.DataFrame({'Logistic Regression': logisticregression, 'Decision Tree': decisiontree, 'Random Forest': randomforest,'Ada Boost':s_abm,'XGBOOST':s_xgb})\n",
    "df_complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Considering F1 - Score as the metric we can say that XG-Boost works best(But XG-Boost is bound to do well because of its complexity) and for such small data better to use Logistic Regression or Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chithsabesh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Chithsabesh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Chithsabesh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Chithsabesh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Chithsabesh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Chithsabesh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Chithsabesh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Chithsabesh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Chithsabesh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Chithsabesh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Chithsabesh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Chithsabesh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Chithsabesh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Chithsabesh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Chithsabesh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chithsabesh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Chithsabesh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Chithsabesh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Chithsabesh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Chithsabesh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Chithsabesh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                          fit_intercept=True,\n",
       "                                          intercept_scaling=1, l1_ratio=None,\n",
       "                                          max_iter=100, multi_class='auto',\n",
       "                                          n_jobs=None, penalty='l2',\n",
       "                                          random_state=None, solver='lbfgs',\n",
       "                                          tol=0.0001, verbose=0,\n",
       "                                          warm_start=False),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid={'C': [0.001, 0.1, 1, 10, 100]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "grid = GridSearchCV(estimator = LogisticRegression(), cv = 5, param_grid = {'C': [0.001, 0.1, 1, 10, 100]})\n",
    "grid.fit(x_train_smote, y_train_smote)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_grid  = grid.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loggrid   = pd.DataFrame({'actual': test_y,\n",
    "                         'predicted': log_grid})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160 644 85 239\n"
     ]
    }
   ],
   "source": [
    "tp_grid = df_loggrid[(df_loggrid['predicted']=='Y') & (df_loggrid['actual']=='Y')].shape[0]\n",
    "tn_grid = df_loggrid[(df_loggrid['predicted']=='N') & (df_loggrid['actual']=='N')].shape[0]\n",
    "fp_grid = df_loggrid[(df_loggrid['predicted']=='N') & (df_loggrid['actual']=='Y')].shape[0]\n",
    "fn_grid = df_loggrid[(df_loggrid['predicted']=='Y') & (df_loggrid['actual']=='N')].shape[0]\n",
    "print(tp_grid,tn_grid,fp_grid,fn_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40100250626566414\n",
      "0.8834019204389575\n",
      "0.6530612244897959\n",
      "0.4968944099378882\n"
     ]
    }
   ],
   "source": [
    "sensitivity_grid = tp_grid/(tp_grid+fn_grid)\n",
    "print(sensitivity_grid)\n",
    "specificity_grid = tn_grid/(tn_grid+fp_grid)\n",
    "print(specificity_grid)\n",
    "precision_grid = tp_grid/(tp_grid+fp_grid)\n",
    "print(precision_grid)\n",
    "f1_score_grid = 2*(precision_grid*sensitivity_grid)/(precision_grid+sensitivity_grid)\n",
    "print(f1_score_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid search CV didnt improve results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [10, 20, 30, 40],\n",
    "    'max_features': [2, 3],\n",
    "    'min_samples_leaf': [3, 4, 5],\n",
    "    'min_samples_split': [8, 10, 12],\n",
    "    'n_estimators': [100, 200, 300, 1000]\n",
    "}\n",
    "# Create a based model\n",
    "rf = RandomForestClassifier()\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "                          cv = 3, n_jobs = -1, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 288 candidates, totalling 864 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    7.8s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   35.3s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 864 out of 864 | elapsed:  3.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'max_depth': 30,\n",
       " 'max_features': 3,\n",
       " 'min_samples_leaf': 3,\n",
       " 'min_samples_split': 10,\n",
       " 'n_estimators': 1000}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the grid search to the data\n",
    "grid_search.fit(x_train_smote, y_train_smote)\n",
    "grid_search.best_params_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit random forest for this parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rf_grid = RandomForestClassifier(random_state=1,n_estimators=300,max_features=3,max_depth=30,bootstrap=True,min_samples_leaf=3,min_samples_split=8)\n",
    "model_rf_grid.fit(x_train_smote,y_train_smote)\n",
    "\n",
    "test_pred_rf_grid = model_rf_grid.predict(test_x)\n",
    "grid_rf   = pd.DataFrame({'actual': test_y,\n",
    "                         'predicted': test_pred_rf_grid})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131 727 114 156\n"
     ]
    }
   ],
   "source": [
    "tp_rf_grid = grid_rf[(grid_rf['predicted']=='Y') & (grid_rf['actual']=='Y')].shape[0]\n",
    "tn_rf_grid = grid_rf[(grid_rf['predicted']=='N') & (grid_rf['actual']=='N')].shape[0]\n",
    "fp_rf_grid = grid_rf[(grid_rf['predicted']=='N') & (grid_rf['actual']=='Y')].shape[0]\n",
    "fn_rf_grid = grid_rf[(grid_rf['predicted']=='Y') & (grid_rf['actual']=='N')].shape[0]\n",
    "print(tp_rf_grid,tn_rf_grid,fp_rf_grid,fn_rf_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4564459930313589\n",
      "0.8644470868014269\n",
      "0.5346938775510204\n",
      "0.4924812030075188\n"
     ]
    }
   ],
   "source": [
    "sensitivity_rf_grid = tp_rf_grid/(tp_rf_grid+fn_rf_grid)\n",
    "print(sensitivity_rf_grid)\n",
    "specificity_rf_grid = tn_rf_grid/(tn_rf_grid+fp_rf_grid)\n",
    "print(specificity_rf_grid)\n",
    "precision_rf_grid = tp_rf_grid/(tp_rf_grid+fp_rf_grid)\n",
    "print(precision_rf_grid)\n",
    "f1_score_rf_grid=2*(precision_rf_grid*sensitivity_rf_grid)/(precision_rf_grid+sensitivity_rf_grid)\n",
    "print(f1_score_rf_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning the Random forest increased the performance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building models using Injury column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Chithsabesh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "attacks_injury = attacks[['Injury','Is_Fatal']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = attacks_injury['Injury'].str.lower().str.replace('[^a-z ]', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "len(stopwords)\n",
    "stopwords[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['minor injuri thigh', 'lacer hand']\n"
     ]
    }
   ],
   "source": [
    "stemmer = nltk.stem.PorterStemmer()\n",
    "docs_clean = []\n",
    "for doc in docs.head(2):\n",
    "    words = doc.split(' ')\n",
    "    #print(doc)\n",
    "    #print(words)\n",
    "    words_clean = []\n",
    "    for word in words:\n",
    "        if word not in stopwords:\n",
    "            words_clean.append(stemmer.stem(word))\n",
    "    \n",
    "    doc_clean = ' '.join(words_clean)\n",
    "    docs_clean.append(doc_clean)\n",
    "    \n",
    "print(docs_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = attacks['Injury'].str.lower().str.replace('[^a-z ]', '')\n",
    "\n",
    "def clean_sentence(text):\n",
    "    words = text.split(' ')\n",
    "    words_clean = [stemmer.stem(word) for word in words if word not in stopwords]\n",
    "    return ' '.join(words_clean)\n",
    "docs_clean = docs.apply(clean_sentence)\n",
    "#docs_clean.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Term Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3760x871 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 14601 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(docs_clean)\n",
    "dtm = vectorizer.transform(docs_clean)\n",
    "dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3760,)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99.55416249358771"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_of_zeros = (3760 * 871) - 14601\n",
    "sparsity = (no_of_zeros) / (3760*871) * 100\n",
    "sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3260359"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dtm = pd.DataFrame(dtm.toarray(),\n",
    "                     columns = vectorizer.get_feature_names())\n",
    "(df_dtm == 0).sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_x, test_x = train_test_split(df_dtm,\n",
    "                                   test_size=0.2,\n",
    "                                   random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = attacks_injury.iloc[train_x.index]['Is_Fatal']\n",
    "test_y  = attacks_injury.iloc[test_x.index]['Is_Fatal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "model = GaussianNB()\n",
    "model.fit(train_x, train_y)\n",
    "predict_class = model.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gnb  = pd.DataFrame({'actual': test_y,\n",
    "                         'predicted': predict_class})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152 168 6 384\n"
     ]
    }
   ],
   "source": [
    "tp_gnb = df_gnb[(df_gnb['predicted']=='Y') & (df_gnb['actual']=='Y')].shape[0]\n",
    "tn_gnb = df_gnb[(df_gnb['predicted']=='N') & (df_gnb['actual']=='N')].shape[0]\n",
    "fp_gnb = df_gnb[(df_gnb['predicted']=='N') & (df_gnb['actual']=='Y')].shape[0]\n",
    "fn_gnb = df_gnb[(df_gnb['predicted']=='Y') & (df_gnb['actual']=='N')].shape[0]\n",
    "print(tp_gnb,tn_gnb,fp_gnb,fn_gnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2835820895522388\n",
      "0.9655172413793104\n",
      "0.9620253164556962\n",
      "0.43804034582132567\n"
     ]
    }
   ],
   "source": [
    "sensitivity_gnb = tp_gnb/(tp_gnb+fn_gnb)\n",
    "print(sensitivity_gnb)\n",
    "specificity_gnb = tn_gnb/(tn_gnb+fp_gnb)\n",
    "print(specificity_gnb)\n",
    "precision_gnb = tp_gnb/(tp_gnb+fp_gnb)\n",
    "print(precision_gnb)\n",
    "f1_score_gnb=2*(precision_gnb*sensitivity_gnb)/(precision_gnb+sensitivity_gnb)\n",
    "print(f1_score_gnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multinomial NB\n",
    "model = MultinomialNB()\n",
    "model.fit(train_x, train_y)\n",
    "predict_class = model.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mnb  = pd.DataFrame({'actual': test_y,\n",
    "                         'predicted': predict_class})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158 583 1 5\n"
     ]
    }
   ],
   "source": [
    "tp_mnb = df_mnb[(df_mnb['predicted']=='Y') & (df_mnb['actual']=='Y')].shape[0]\n",
    "tn_mnb = df_mnb[(df_mnb['predicted']=='N') & (df_mnb['actual']=='N')].shape[0]\n",
    "fp_mnb = df_mnb[(df_mnb['predicted']=='N') & (df_mnb['actual']=='Y')].shape[0]\n",
    "fn_mnb = df_mnb[(df_mnb['predicted']=='Y') & (df_mnb['actual']=='N')].shape[0]\n",
    "print(tp_mnb,tn_mnb,fp_mnb,fn_mnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9693251533742331\n",
      "0.9982876712328768\n",
      "0.9937106918238994\n",
      "0.9813664596273292\n"
     ]
    }
   ],
   "source": [
    "sensitivity_mnb = tp_mnb/(tp_mnb+fn_mnb)\n",
    "print(sensitivity_mnb)\n",
    "specificity_mnb = tn_mnb/(tn_mnb+fp_mnb)\n",
    "print(specificity_mnb)\n",
    "precision_mnb = tp_mnb/(tp_mnb+fp_mnb)\n",
    "print(precision_mnb)\n",
    "f1_score_mnb=2*(precision_mnb*sensitivity_mnb)/(precision_mnb+sensitivity_mnb)\n",
    "print(f1_score_mnb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial Naive bayes is excellent for classifying Fatality with just one Text Column"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
